{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Science:  Midterm\n",
    "\n",
    "Please complete all of the questions. Save your notebook as html->pdf, and submit to GradeScope. You may use any materials to solve the problems, including past Labs and the Internet. Have fun!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# direct plots to appear within the cell, and set their style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyzing a Gutenberg book\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/en/a/a3/Doriangray.jpg\" style=\"width: 200px; margin:0px 20px 20px 0; margin-top: 10px\"/>](https://en.wikipedia.org/wiki/The_Picture_of_Dorian_Gray)\n",
    "\n",
    "\n",
    "In this problem we'll work with the book \"The Picture of Dorian Gray\" from the Gutenberg project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "import re\n",
    "def read_url(url): \n",
    "    return re.sub('\\\\s+', ' ', urlopen(url).read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The online book for \"The Picture of Dorian Gray,\" by Oscar Wilde, is [here](https://www.gutenberg.org/ebooks/174).\n",
    "From this web site you can see various metadata for the book as well as the [link the text itself](http://www.gutenberg.org/cache/epub/174/pg174.txt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dorian_gray_url = 'http://www.gutenberg.org/cache/epub/174/pg174.txt'\n",
    "dorian_gray_text = read_url(dorian_gray_url)\n",
    "dorian_gray_chapters = dorian_gray_text.split('CHAPTER ')[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions, give a Python expression that evaluates to the correct answer -- do not \"hard code\" the answer with a number or constant expression like `42` or `\"forty-two\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean string function\n",
    "\n",
    "Write a function `clean_text(str)` that removes all characters that are not letters or space or double quotation marks (\"), and removes extra spaces. For example, `clean_text('\"This\"  is a...    test!')` should return the string `'\"This\" is a test'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(str):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a (cute) way to check that your function is doing things properly\n",
    "# Run this line, and it will check whether or not your function satisfies our criteria\n",
    "# If nothing appears, then you're good to go\n",
    "assert(clean_text('\"This\"  is a...    test!') == '\"This\" is a test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process each chapter\n",
    "\n",
    "Apply the `clean_text` function to clean up each chapter, that is, each element of the list `dorian_gray_chapters`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in np.arange(len(dorian_gray_chapters)):\n",
    "    dorian_gray_chapters[c] = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of words by chapter\n",
    "\n",
    "Complete the following function so that it returns an array of counts of the word `word` in each of the chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_of_word(word):\n",
    "    count = []\n",
    "    for c in np.arange(len(dorian_gray_chapters)):\n",
    "        count.append(...)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts of names\n",
    "\n",
    "Form a table `name_counts` with a column called 'chapter' with the chapter number,\n",
    "and column containing the word counts for each of `Dorian`, `Sibyl`, `Lord Henry`, `Basil` and `Alan`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_counts = Table().with_columns('chapter', np.arange(1, len(dorian_gray_chapters)+1))\n",
    "...\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the chapter lengths\n",
    "\n",
    "Give a bar plot or line plot showing the number of occurrences of each of these characters by chapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Advanced (Extra Credit)\n",
    "\n",
    "Move onto Question 2 first before working through this section. Note that if you get stuck on this first problem, you can move onto the second one, as I've given you a copy of the solutions.\n",
    "\n",
    "### Splitting Spoken and Written\n",
    "\n",
    "It might help to take a quick peek at the original text: http://www.gutenberg.org/cache/epub/174/pg174.txt. An important distiction is spoken words versus just written words. That is, spoken words are those spoken explicitly by some character in the novel, and written words are everything else.\n",
    "\n",
    "We are actually going to shift gears here and work not with the chapter level data, but with paragraph level data. As always, print out the first few elements of `dg_para` to get a feel for what the variable is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_para = urlopen(dorian_gray_url).read().decode().split(\"\\r\\n\\r\\n\")[27:]\n",
    "dg_para = [re.sub(\"\\s+\", \" \", p) for p in dg_para if not re.search(\"CHAPTER\", p)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working from `dg_para`, create two new lists `dg_spoken_para` and `dg_written_para`, where the elements of the list correspond to the spoken and written words found in that paragraph, respectively. That is, `dg_written_para[0]` is a string of all the non-spoken words in the first paragraph of the text (in this particular case it will be the same as `dg_para[0]`).\n",
    "\n",
    "Hint: it might help to refresh your memory on slicing (https://docs.python.org/2.3/whatsnew/section-slices.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who Mentions Whom\n",
    "\n",
    "*This is an open-ended, messy question that's much more like real life. Even if you didn't complete the previous task, you can still work on this task. I've included some code (that's been commented out) that you can run to get the correct variables.*\n",
    "\n",
    "Ideally, it would be great if we could determine who is talking to whom during these spoken interactions in the text. Have a look at the original text, and you should be able to see some patterns that might hint at ways to go about doing that. Let's consider the *somewhat* simpler problem of seeing how many times a person mentions another person when speaking.\n",
    "\n",
    "Your goal is to collect such information. That is, what I want to end up with are pairs of names, say (Dorian, Basil), and each pair corresponds to an instance where you believe Dorian is speaking, and mentions Basil. Clearly, this is impossible to get perfect. The goal here is to just get a rough estimate. Work with the same list of names as before: `Dorian`, `Sibyl`, `Lord Henry`, `Basil` and `Alan`.\n",
    "\n",
    "Here are some ideas to get you started. Feel free to come up with your own idea!\n",
    "\n",
    "1. Per paragraph, count the number of times a name is mentioned in the written part, and just pick the name that appeared the most number of times, and just treat that as the \"speaker\".\n",
    "\n",
    "2. Use the pattern matching techniques from `Lab 03` to pinpoint who's actually speaking.\n",
    "\n",
    "For instance, the 5th paragraph is\n",
    "\n",
    "> \"It is your best work, Basil, the best thing you have ever done,\" said\n",
    "Lord Henry languidly.  \"You must certainly send it next year to the\n",
    "Grosvenor.  The Academy is too large and too vulgar.  Whenever I have\n",
    "gone there, there have been either so many people that I have not been\n",
    "able to see the pictures, which was dreadful, or so many pictures that\n",
    "I have not been able to see the people, which was worse.  The Grosvenor\n",
    "is really the only place.\"\n",
    "\n",
    "That should give me an entry: (Lord Henry, Basil).\n",
    "\n",
    "Note that, just like in `Lab 05`, what I actually want are two lists: `speaker` and `subject` of the same length, so that each entry is the pair I just described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you weren't able to solve the previous task, comment out the next three lines, and work with these variables\n",
    "# import pickle\n",
    "# dg_written_para = pickle.load(open(\"dg_w.p\", \"rb\"))\n",
    "# dg_spoken_para = pickle.load(open(\"dg_s.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing TED Talks\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/42/Chris_Anderson_2007.jpg\" style=\"width: 300px; margin:0px 20px 20px 0; margin-top: 10px\"/>](https://goo.gl/3wV6Kb)\n",
    "\n",
    "Now you will carry out some text processing on 50 of the TED talks. The following bit of code reads in the data and creates a subject line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ted_talks_50.csv\"\n",
    "data = Table.read_table(filename)\n",
    "urls = data.column(1)\n",
    "subjects = [re.sub('_', ' ', re.sub('.*/','', url)) for url in urls]\n",
    "ted = Table().with_columns('subject', subjects, 'transcript', data.column('transcript'))\n",
    "ted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest talk\n",
    "\n",
    "Which talk is the longest, in terms of number of characters in the transcript? The shortest? Give the name of the talk, as in the example below. To do this, you might want to create a column containing the length of each talk, and then sort on this column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_talk = ted.column('subject')[0]\n",
    "last_talk = ted.column('subject')[-1]\n",
    "longest_talk = ...\n",
    "shortest_talk = ...\n",
    "\n",
    "print(\"First talk: `%s`\" % first_talk)\n",
    "print(\"Last talk: `%s`\" % last_talk)\n",
    "print(\"Longest talk: `%s`\" % longest_talk)\n",
    "print(\"Shortest talk: `%s`\" % shortest_talk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find names\n",
    "\n",
    "Write a function `find_all_names` that takes an input string and returns all occurrences of two consecutive upper-case words in the string. So `find_names('Mickey Mouse is a friend of Donald Duck and Donald Duck likes Mickey too!')` should return `['Mickey Mouse', 'Donald Duck', 'Donald Duck']`. Use the function `re.findall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_names(str):\n",
    "    return re.findall(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a name list\n",
    "\n",
    "Recall that we can use the `Counter` class to build the vocabulary.  Counter is a Python dictionary where keys are the objects to be counted and the values are their counts. Using the `Counter` and the `find_names` function, create a vocabulary of all unique proper names that appear in the talk. How many names are there? Print them out sorted order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = Counter()\n",
    "for i in np.arange(50):\n",
    "    # find all the names in talk number i, and add them to names with the update function from Counter\n",
    "    ...\n",
    "\n",
    "num_names = len(names)\n",
    "print(num_names)\n",
    "# sorted(names) will sort them lexicographically\n",
    "sorted(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common\n",
    "\n",
    "What are the ten most common names in these Ted Talks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_most_common = ...\n",
    "ten_most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names by talk\n",
    "\n",
    "Now create a column `names mentioned` that lists the *unique* names mentioned in each talk. *Hint:* You might try converting a list of names to a set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "talk_names = []\n",
    "for i in np.arange(50):\n",
    "    this_talk_names = ...\n",
    "    talk_names.append(this_talk_names)\n",
    "\n",
    "ted = ted.with_column('names mentioned', talk_names)\n",
    "ted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the names\n",
    "\n",
    "What are some problems with this method of finding names? Give some examples of non-names that are found. How would you improve the method? Describe how you would implement the improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
