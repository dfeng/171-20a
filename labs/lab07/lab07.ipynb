{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: Movie Plots (2/2)\n",
    "In this lab we will continue working with topic models, but this time with a new dataset. Instead of abstracts of scientific articles, we will create topic models over movie plot descriptions. This is a dataset containing descriptions of movies from Wikipedia. The dataset was [obtained](https://www.kaggle.com/jrobischon/wikipedia-movie-plots) from Kaggle, an online community of data scientists.\n",
    "\n",
    "We will review the Counter class and how to build vocabularies again, and get practice with building topic models for new datasets. We'll also introduce the concepts of stemming and lemmatization. \n",
    "\n",
    "Spoiler alert! We will use the movie \"[Husbands and Wives](https://en.wikipedia.org/wiki/Husbands_and_Wives)\" as a running example...\n",
    "\n",
    "[<img width=200 src=\"https://upload.wikimedia.org/wikipedia/en/7/70/Husbands_moviep.jpg\">](https://en.wikipedia.org/wiki/Husbands_and_Wives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logging.root.level = logging.CRITICAL \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# direct plots to appear within the cell, and set their style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, the movie plot descriptions are in a CSV format in `movie_plots.csv`. The file is hosted on the Amazon Web Service s3. We'll use the `datascience` package to read this CSV file.\n",
    "\n",
    "The CSV file is rather large, so we'll randomly sample only 10000 rows, skipping over the others during the loading process. We could have sampled the rows after loading the entire CSV file, but this method saves memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "total_size = 34886\n",
    "sample_size = 10000\n",
    "sample_rows = np.random.choice(range(1,total_size+2), sample_size, replace=False)\n",
    "skip_rows = np.setdiff1d(range(1,total_size+2), sample_rows)\n",
    "\n",
    "filename = \"https://s3.amazonaws.com/sds171/labs/lab07/movie_plots.csv\"\n",
    "data = Table.read_table(filename, skiprows=skip_rows)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now in the `Table` format, which we are familiar with. We are only concerned with the plots, so we will extract them from the `Table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 3402\n",
    "\n",
    "plots = data.column('Plot')\n",
    "    \n",
    "print(\"Number of documents: %d\\n\" % len(plots))\n",
    "print(plots[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot description is from the movie \"[Husbands and Wives](https://en.wikipedia.org/wiki/Husbands_and_Wives)\"\n",
    "\n",
    "We don't have LaTeX markup in these documents, but we'll still use some regular expressions to do some simpe pre-processing of punctuation. There are lots of names in the plot descriptions, so we'll remove all the words that have a capitalized first letter. This will remove lots of non-name words as well, but this'll be sufficient for our goal of building a basic topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace '-' with ' ', then remove punctuation\n",
    "plots = [re.sub('-', ' ', plot) for plot in plots]\n",
    "plots = [re.sub('[^\\w\\s]', '', plot) for plot in plots]\n",
    "\n",
    "# remove tokens with a capitalized first letter \n",
    "# (broad stroke to remove names)\n",
    "plots = [re.sub('[A-Z]\\w*', '', plot) for plot in plots]\n",
    "# replace multiple spaces by a single space\n",
    "plots = [re.sub('[ ]+', ' ', plot) for plot in plots]\n",
    "\n",
    "print(plots[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we further process each plot description by converting it to lower case, stripping leading and trailing white space, and then tokenizing by splitting on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_tok = []\n",
    "for plot in plots:\n",
    "    processed = plot.lower().strip().split(' ')\n",
    "    plots_tok.append(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Lab06, we will remove tokens that have digits, possessives or contractions, or are empty strings.\n",
    "- `is_numeric(string)` checks if `string` has any numbers\n",
    "- `has_poss_contr(string)` checks if `string` has possessives or contractions\n",
    "- `empty_string(string)` checks if `string` is an empty string\n",
    "- `remove_string(string)` checcks if `string` should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(string):\n",
    "    for char in string:\n",
    "        if char.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_poss_contr(string):\n",
    "    for i in range(len(string) - 1):\n",
    "        if string[i] == '\\'' and string[i+1] == 's':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def empty_string(string):\n",
    "    return string == ''\n",
    "\n",
    "def remove_string(string):\n",
    "    return is_numeric(string) | has_poss_contr(string) | empty_string(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    filtered = []\n",
    "    for token in plot:\n",
    "        if not remove_string(token):\n",
    "            filtered.append(token)\n",
    "    temp.append(filtered)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that to build topic models, we require the following components:\n",
    "- A vocabulary of tokens that appear across all documents.\n",
    "- A mapping of those tokens to a unique integer identifier, because topic model algorithms treat words by these identifiers, and not the strings themselves. For example, we represent `'epidemic'` as `word2id['epidemic'] = 50`\n",
    "- The corpus, where each document in the corpus is a collection of tokens, where each token is represented by the identifier and the number of times it appears in the document. For example, in the first document above the token `'epidemic'`, which appears twice, is represented as `(50, 2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will build a vocabulary representing the tokens that have appeared across all the plot descriptions we have. When doing this, we typically want to (1) remove rare words, (2) remove stop words and (3) stem/lemmatize words.\n",
    "\n",
    "Stemming and lemmatization are techniques to derive the root form of a word (`'run'`), given an inflected form of the word (`'running'`).\n",
    "\n",
    "**Stemming**. This is the process of shortening a word based on common heuristics, such as removing suffixes. For example, `'appeared'` -> `'appear'`. Sometimes, there is nothing to remove, like in `'saw'` -> `'saw'`.\n",
    "\n",
    "**Lemmatization**. This is the process of deriving the root word based on its part of speech and word morphology. For example, `'saw'` -> `'saw'` if it is used as a noun, but `'saw'` -> `'see'` if it is used as a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    processed = []\n",
    "    for token in plot:\n",
    "        processed.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "    temp.append(processed)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can use the `Counter` class to build the vocabulary. The `Counter` is an extension of the Python dictionary, and also has key-value pairs. For the `Counter`, keys are the objects to be counted, while values are their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "for plot in plots_tok:\n",
    "    vocab.update(plot)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that removing rare words helps prevent our vocabulary from being too large. Many tokens appear only a few times across all the plot descriptions. Keeping them in the vocabulary increases subsequent computation time. Furthermore, their presence tends not to carry much significance for a document, since they can be considered as anomalies.\n",
    "\n",
    "We remove rare words by only keeping tokens that appear more than 25 times across all plot descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for token in vocab.elements():\n",
    "    if vocab[token] > 25:\n",
    "        tokens.append(token)\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that stop words are defined as very common words such as `'the'` and `'a'`. Removing stop words is important because their presence also does not carry much significance, since they appear in all kinds of texts.\n",
    "\n",
    "We will remove stop words by removing the 200 most common tokens across all the plot descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "for item in vocab.most_common(200):\n",
    "    stop_word = item[0]\n",
    "    stop_words.append(stop_word)\n",
    "tokens = []\n",
    "for token in vocab.elements():\n",
    "    if token not in stop_words:\n",
    "        tokens.append(token)\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a mapping for tokens to unique identifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = vocab.items()\n",
    "id2word = {}\n",
    "word2id = {}\n",
    "idx = 0\n",
    "for word, count in vocab.items():\n",
    "    id2word[idx] = word\n",
    "    word2id[word] = idx\n",
    "    idx += 1\n",
    "    \n",
    "print(\"Number of tokens mapped: %d\" % len(id2word))\n",
    "print(\"Identifier for 'photograph': %d\" % word2id['photograph'])\n",
    "print(\"Word for identifier %d: %s\" % (word2id['photograph'], id2word[word2id['photograph']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will remove, for each plot description, the tokens that are not found in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    filtered = []\n",
    "    for token in plot:\n",
    "        if token in vocab:\n",
    "            filtered.append(token)\n",
    "    temp.append(filtered)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the corpus. Recall that for the tokens in a plot description:\n",
    "\n",
    "```\n",
    "['doctor', 'pearson', 'michael', 'rennie', 'canadian', 'hospital', 'province', 'quebec', 'receive', 'series', 'poison', 'pen', 'letter', 'letter', 'sign', 'mysterious', 'picture', 'feather', 'deliver', 'others', 'small', 'canadian', 'cora', 'laurent', 'constance', 'smith', 'main', 'doctor', 'dr', 'laurent', 'charles', 'boyer', 'hospital', 'receive', 'letter', 'accuse', 'affair', 'pearson', 'letter', 'inform', 'shell', 'shock', 'veteran', 'mr', 'cancer', 'distraught', 'commit', 'suicide', 'quickly', 'townsfolk', 'point', 'finger', 'possible', 'suspect'] \n",
    "```\n",
    "the corpus has the format\n",
    "```\n",
    "[(1841, 2), (2095, 2), (2096, 1), (2097, 1), (2098, 2), (105, 2), (2099, 1), (2100, 1), (270, 2), (1763, 1), (1870, 1), (2101, 1), (2017, 4), (633, 1), (1270, 1), (1093, 1), (2102, 1), (1197, 1), (113, 1), (1583, 1), (2103, 1), (2104, 2), (2105, 1), (873, 1), (1950, 1), (107, 1), (2106, 1), (2107, 1), (116, 1), (1436, 1), (62, 1), (2108, 1), (213, 1), (2109, 1), (1205, 1), (2110, 1), (1042, 1), (1275, 1), (1259, 1), (1342, 1), (2111, 1), (440, 1), (1662, 1), (374, 1), (663, 1)]\n",
    "```\n",
    "\n",
    "where each element is a pair containing the identifier for the token and the count of that token in just that plot description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for plot in plots_tok:\n",
    "    plot_count = Counter(plot)\n",
    "    corpus_doc = []\n",
    "    for item in plot_count.items():\n",
    "        pair = (word2id[item[0]], item[1])\n",
    "        corpus_doc.append(pair)\n",
    "    corpus.append(corpus_doc)\n",
    "\n",
    "print(\"Plot, tokenized:\\n\", plots_tok[sample], \"\\n\")\n",
    "print(\"Plot, in corpus format:\\n\", corpus[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to create our topic model!\n",
    "\n",
    "We again use gensim, a Python library to create topic models. Also, we again use the algorithm called latent dirichlet allocation implemented in the gensim library. \n",
    "\n",
    "**This step takes about 40s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the topic model, we want to view the 10 topics. The topics are represented as a combination of keywords with corresponding weight on the keyword. Note that the order of these topics can change between different training runs of the topic model, since there is no ordering between topics and gensim returns them in an arbitrary order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "num_words = 15\n",
    "top_words = Table().with_column('word rank', np.arange(1,num_words+1))\n",
    "for k in np.arange(num_topics): \n",
    "    topic = lda_model.get_topic_terms(k, num_words)\n",
    "    words = [id2word[topic[i][0]] for i in np.arange(num_words)]\n",
    "    probs = [topic[i][1] for i in np.arange(num_words)]\n",
    "    top_words = top_words.with_column('topic %d' % k, words)\n",
    "    \n",
    "top_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the probability distribution for a given plot description in the `corpus`. This represents how likely it is for the plot description to belong to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 3402\n",
    "lda_model.get_document_topics(corpus[sample], minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's represent this as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dist = lda_model.get_document_topics(corpus[sample], minimum_probability=0)\n",
    "topics = [pair[0] for pair in topic_dist]\n",
    "probabilities = [pair[1] for pair in topic_dist]\n",
    "topic_dist_table = Table().with_columns('Topic', topics, 'Probabilities', probabilities)\n",
    "topic_dist_table.show(20)\n",
    "t = np.argmax(probabilities)\n",
    "print(\"Topic with highest probability: %d (%f)\" % (t, probabilities[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(11,4)\n",
    "plt.bar(topic_dist_table.column('Topic'), topic_dist_table.column('Probabilities'), align='center', alpha=1, color='salmon')\n",
    "plt.xlabel('topic')\n",
    "plt.ylabel('probability')\n",
    "plt.title('Per Topic Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems the movie plot description\n",
    "\n",
    "```\n",
    "The film is about two couples: Jack (Pollack) and Sally (Davis), and Gabe (Allen) and Judy (Farrow). The film starts when Jack and Sally arrive at Gabe and Judy's apartment and announce their separation. Gabe is shocked, but Judy takes the news personally and is very hurt. Still confused, they go out for dinner at a Chinese restaurant.\n",
    "A few weeks later Sally goes to the apartment of a colleague. They plan to go out together to the opera and then to dinner. Sally asks if she can use his phone, and calls Jack. Learning from him that he has met someone, she accuses him of having had an affair during their marriage.\n",
    "Judy and Gabe are introduced to Jack's new girlfriend, Sam, an aerobics trainer. While Judy and Sam shop, Gabe calls Jack's new girlfriend a \"cocktail waitress\" and tells him that he is crazy for leaving Sally for her. About a week later, Judy introduces Sally to Michael (Neeson), Judy's magazine colleague who she clearly is interested in herself. Michael asks Sally out, and they begin dating; Michael is smitten, but Sally is dissatisfied with the relationship.\n",
    "Meanwhile, Gabe has developed a friendship with a young student of his, Rain, and has her read the manuscript of his novel. She comments on its brilliance, but has several criticisms, to which Gabe reacts defensively.\n",
    "At a party, Jack learns from a friend that Sally is seeing someone, and flies into a jealous rage. He and Sam break up after an intense argument, and Jack drives back to his house to find Sally in bed with Michael. He asks Sally to give their marriage another chance, but she tells him to leave.\n",
    "Less than two weeks later, however, Jack and Sally are back together and the couple meet Judy and Gabe for dinner like old times. After dinner, Judy and Gabe get into an argument about her not sharing her poetry. After Gabe makes a failed pass at her, Judy tells him she thinks the relationship is over; a week later Gabe moves out. Judy begins seeing Michael.\n",
    "Gabe goes to Rain's 21st birthday party, and gives her a music box as a present. She asks him to kiss her, and though the two share a romantic moment, Gabe tells her they should not pursue it any further. As he walks home in the rain, he realizes that he has ruined his relationship with Judy.\n",
    "Michael tells Judy he needs time alone, then says he can't help still having feelings for Sally. Angry and hurt, Judy walks out into the rain. Highlighting her \"passive aggressiveness,\" Michael follows and begs her to stay with him. A year and a half later they marry.\n",
    "At the end, the audience sees a pensive Jack and Sally back together. Jack and Sally admit their marital problems still exist (her frigidity is not solved), but they find they accept their problems as simply the price they have to pay to remain together.\n",
    "Gabe is living alone because he says he is not dating for the time being, as he does not want to hurt anyone. The film ends with an immediate cut to black after Gabe asks the unseen documentary crew, \"Can I go? Is this over?\"\n",
    "```\n",
    "\n",
    "has the greatest likelihood to fall under the topic number with topic relating to relationships, which matches our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models for TED Talk Transcripts\n",
    "\n",
    "*Your turn!* Your task is broken down into two parts.\n",
    "\n",
    "#### 1. Run Topic Models\n",
    "You are given a new dataset of transcripts of TED Talks. This dataset is also [obtained](https://www.kaggle.com/rounakbanik/ted-talks) from Kaggle. You can use the following link to load the data, in a CSV file `ted_talks.csv`, from s3:\n",
    "\n",
    "```\n",
    "filename = \"https://s3.amazonaws.com/sds171/labs/lab07/ted_talks.csv\"\n",
    "data = Table.read_table(filename)\n",
    "```\n",
    "\n",
    "Run the code above to train a topic model over this new dataset. In particular,\n",
    "- Load the CSV file. You can simply use the two lines of code above, without skipping any rows.\n",
    "- Preprocess\n",
    "    - Discard some transcripts if you find it appropriate.\n",
    "    - Add a regular expression here if you find it is appropriate.\n",
    "    - Tokenize and remove numerics, possessives/contractions, and empty strings\n",
    "    - Lemmatize tokens\n",
    "- Create a vocabulary using a `Counter`\n",
    "- Filter the vocabulary by removing rare words and stop words. You should be getting a final vocabulary size of about 7000.\n",
    "- Create the identifier mappings `word2id` and `id2word`\n",
    "- Filter tokens from the tokenized data using the final vocabulary\n",
    "- Create the corpus\n",
    "- Train the topic model\n",
    "- Show the topics along with the top words\n",
    "\n",
    "You should find yourself simply using the code given above for this new dataset. Very few changes are necessary!\n",
    "\n",
    "\n",
    "#### 2. Discussion\n",
    "Discuss the results. Choose two or three articles and describe how the most probable topic does or does not seem to accurately represent the main theme of the paper. Include your comments in Markdown cell, with code cells added as needed to pull out particular rows of your table. You may find it useful to copy over the code for `create_topic_table` from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
