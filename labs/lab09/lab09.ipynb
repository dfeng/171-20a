{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09: Word embeddings (2/2)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img width=300 align=\"left\" margin=\"20\" src=\"number-matrix.jpg\">\n",
    "<img width=300 margin=\"20\" src=\"wordgeom.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "In this lab we will continue to work with word embeddings&mdash;machine learning algorithms that represent categorical data like words as vectors in a high dimensional space. \n",
    "\n",
    "Embeddings are constructed using cooccurrence statistics, and can be applied whenever you have \"objects\" that appear together&mdash;songs in playlists, words in documents, ingredients in recipes... Embeddings can reveal surprising semantic relations encoded in linear relationships; but they are \"data hungry\" and require large corpora of text or other coocurrence data to construct useful representations.\n",
    "\n",
    "We will first explore how to visualize the embedding vectors. Perhaps confusingly, this is done by embedding the embedding vectors themselves into two dimensions. The most \n",
    "popular method for doing this is called t-SNE (\"TEE snee\"). [Here](https://lvdmaaten.github.io/tsne/) is an overview of t-SNE by its creator, and [here](http://cs.stanford.edu/people/karpathy/tsnejs/) is an introduction that includes some (potentially) interesting examples. We will then explore ways in which embeddings may represent certain \"cultural biases,\" which has been a topic of recent interest in the machine learning and AI communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading in the 100-dimensional GloVe embeddings we worked with last time. Recall that these are trained using word cooccurrence counts from about 6 billion words of text from Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gdl\n",
    "from gensim.models import KeyedVectors\n",
    "glove = gdl.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll import the usual modules, and in addition a module that implements t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define two functions. The first takes a pre-trained embedding model, together with a vocabulary of words (as strings), which is a subset of the words in the embedding vocabulary. It then runs t-SNE on that vocabulary, displaying the results as a point cloud. The second function displays the location of particular words, so that we can get a sense of the geometry of the embeddings, and what's close to what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_tsne_for_voc(model, voc):\n",
    "    word_list = [w for w in model.vocab]\n",
    "    w = word_list[0]\n",
    "    dim = len(model[w])\n",
    "    print(\"embedding dimension: %d\" % dim)\n",
    "    print(\"computing t-SNE vectors over %d words...\" % len(voc))\n",
    "    \n",
    "    # collect all the vectors in a list\n",
    "    arr = np.empty((0,dim), dtype='f')\n",
    "    for w in voc:\n",
    "        wrd_vector = model[w]\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    # display scatter plot of the 2-d vectors\n",
    "    plt.figure(figsize=(15,15))    \n",
    "    plt.scatter(Y[:,0], Y[:,1], s=2)\n",
    "    plt.show()\n",
    "    return (Y,voc)\n",
    "\n",
    "\n",
    "# this function displays the \"word cloud\" of all points from the t-SNE vectors,\n",
    "# and then labels the words in the array input_words\n",
    "\n",
    "def display_tsne_words(Z, input_words, size1=2, size2=50, offset=5):\n",
    "    Y = Z[0]\n",
    "    voc = Z[1]\n",
    "    x_coord = Y[:, 0]\n",
    "    y_coord = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.scatter(x_coord, y_coord, s=size1)\n",
    "    for w in np.arange(len(input_words)):\n",
    "        index = voc.index(input_words[w])\n",
    "        plt.scatter(x_coord[index], y_coord[index],s=size2)\n",
    "        plt.annotate(input_words[w], xy=(x_coord[index],y_coord[index]), \\\n",
    "                     xytext=(offset,offset), textcoords='offset points')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bit of code computes word frequencies. We read in the 'text8' dataset from last lab, which you'll recall is a few million words of Wikipedia text. We then build a vocabulary over this data using the `Counter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logging.root.level = logging.CRITICAL \n",
    "\n",
    "s = time.time()\n",
    "voc = Counter()\n",
    "for rec in open ('../lab08/text8', 'r'):\n",
    "    rec = rec.strip()\n",
    "    voc.update(rec.split())\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we grab the most common words, excluding the top (say) 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "words = voc.most_common(n+100)[100:]\n",
    "vocab = [word[0] for word in words]\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run t-SNE on the GloVe embeddings, but restricting to this subset of the model vocabulary. This is only for speed and memory considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filtered_voc = [w for w in vocab if w in glove.vocab]\n",
    "len(filtered_voc)\n",
    "plt.figure(figsize=(15,15))\n",
    "Z = generate_tsne_for_voc(glove, filtered_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we display some words by labeling points in the t-SNE embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tsne_words(Z, input_words=['conservative', 'liberal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "Using the functions defined above, generate several plots show locations of points you choose. Generate a plot, and then describe in a markdown cell why this either makes sense or is puzzling to you. For example, you might find that \"apple\" is not near \"fruit\" but is rather close to \"ibm.\" This should be easy to explain. Find others and comment on what you see.\n",
    "\n",
    "To start, first generate t-SNE vectors for a larger subset of the 400,000 model vocabulary. You might use something like `n=10000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cultural bias in embeddings\n",
    "\n",
    "Embeddings are used in an increasing number of AI and machine learning applications. For example, they are used in placing ads at Facebook, recommending entertainment, and they form the first layer of deep neural networks for many prediction tasks. \n",
    "\n",
    "Recently there has been attention paid to \"bias\" in embeddings. To minimize confusion with statistical terminology, let's refer to this as \"cultural bias.\" Here are pointers to recent literature on this topic: [paper 1](https://arxiv.org/abs/1607.06520), [paper 2] (https://arxiv.org/abs/1810.03611), [some code](https://github.com/tolga-b/debiaswe). And here is an interactive site that allows you to explore [gender bias in embeddings](http://wordbias.umiacs.umd.edu/#). Here is a site that investigates [racial bias in embeddings](http://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/). \n",
    "\n",
    "In this portion of the lab, we will make a first foray into cultural bias in embeddings. To begin, let's consider two analogies. `Scientist is to man as ? is to woman` and `Scientist is to woman as ? is to man`.  So, we're exploring the implicit \"maleness\" and \"femaleness\" of the embedding dimensions of the word \"scientist\". Recall how we compute these analogies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=Warning)\n",
    "\n",
    "glove.most_similar(positive=['scientist', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar(positive=['scientist', 'man'], negative=['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a little function that extracts the top ten words from an analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(pos_words, neg_words):\n",
    "    results = glove.most_similar(positive=pos_words, negative=neg_words)\n",
    "    words = [result[0] for result in results]\n",
    "    return words\n",
    "    \n",
    "print(analogy(['scientist', 'woman'], ['man']))\n",
    "print(analogy(['scientist', 'man'], ['woman']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "Write a function `show_analogy_difference` that takes three words, and runs the analogy with the order of the last two words in different orders. The result is a table with two columns for each of these words. For example, the analogies above would give us this table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Table().with_columns('scientist-woman+man', \\\n",
    "                     ['physicist', 'expert', 'biologist', 'professor', 'researcher', 'geologist', 'engineer', 'astronomer', 'science', 'mathematician'], \\\n",
    "                     'scientist+man-woman', \\\n",
    "                     ['researcher', 'anthropologist', 'sociologist', 'psychologist', 'physicist', 'professor', 'biologist', 'geneticist', 'biochemist', 'expert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the function `show_analogy_difference` that would construct such a table for an arbitrary pair. Show the results for several different triples, with an eye toward locating interesting examples of \"cultural bias\" in the embeddings. Comment on your findings in markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_analogy_difference(a='scientist', b='woman', c='man') :\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
