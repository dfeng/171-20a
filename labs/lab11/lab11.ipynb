{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Sentiment Analysis for Beer Reviews (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[<img width=500 align=\"center\" margin=\"20\" src=\"wc_feel.jpg\">](http://www.recommend.beer/analysis/)\n",
    "\n",
    "\n",
    "In this lab we will continue working with beer reviews for sentiment analysis. We will build better classifiers by using document embeddings and methods like logistic regression and KNN classification.\n",
    "\n",
    "Document embeddings are representations of documents as vectors, and are analogous to word embeddings. They can be constructed from word embeddings by composing them, or by building them in a similar manner to word embeddings.\n",
    "\n",
    "Classifiers are models that take as input a set of features and output a discrete label (a class like 'positive', 'negative', 'neutral'). The input features we will use are the document embeddings of the review. Whereas we previously used rule-based methods based on small lists of words to construct the sentiment classifier, now we will use machine learning methods to approximate a relationship between reviews and their sentiment.\n",
    "\n",
    "We will explore three types of models in this lab:\n",
    "- Document embeddings by taking the mean of word embeddings\n",
    "- Document embeddings by taking a weighted sum of word embeddings\n",
    "- Document embeddings by using Doc2Vec, an algorithm that is similar in spirit to Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, load the usual modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logging.root.level = logging.CRITICAL \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# direct plots to appear within the cell, and set their style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Dataset\n",
    "\n",
    "As before, load the `csv` file that contains our beer reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file and build table\n",
    "filename = \"ratings.csv\"\n",
    "data = Table.read_table(filename)\n",
    "sample_size = data.num_rows\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, transform the review scores from strings like `\"4/5\"` into integers like `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform columns with scoring into ints\n",
    "review_cols = [\"review_appearance\", \"review_aroma\", \"review_overall\", \"review_palate\", \"review_taste\"]\n",
    "\n",
    "def transform_int(score):\n",
    "    return int(re.match(r'([0-9]*)\\/', score)[1])\n",
    "\n",
    "for col in review_cols:\n",
    "    review_score = data.apply(transform_int, col)\n",
    "    data = data.drop(col)\n",
    "    data = data.with_column(col, review_score)\n",
    "\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, label the reviews into 3 classes: positive, neutral and negative. \n",
    "\n",
    "Previously, we took the the first third of reviews sorted by increasing overall review score as labelled negative, the next third neutral, and the last third positive. The problem with this is that a review given a score of, say 15, may be labelled as neutral and another review with that same score may be labelled as positive.\n",
    "\n",
    "A better approach is to find the overall review score associated to the review at the 33rd and 66th percentile, and define our class thresholds according to that score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by increasing review overall score\n",
    "data = data.sort(\"review_overall\")\n",
    "\n",
    "# label class\n",
    "c2i = {\"negative\": 1,\n",
    "       \"neutral\": 2,\n",
    "       \"positive\": 3}\n",
    "\n",
    "# find score associated with tertiles\n",
    "thresh_neg = np.percentile(data.column(\"review_overall\"), q=33)\n",
    "thresh_neu = np.percentile(data.column(\"review_overall\"), q=66)\n",
    "\n",
    "print('negative threshold: %.0f\\nneutral threshold: %.0f\\n' % (thresh_neg, thresh_neu))\n",
    "\n",
    "# label classes\n",
    "def label_class(score, thresh_neg, thresh_neu):\n",
    "    if score <= thresh_neg:\n",
    "        return c2i[\"negative\"]\n",
    "    elif score <= thresh_neu:\n",
    "        return c2i[\"neutral\"]\n",
    "    else:\n",
    "        return c2i[\"positive\"]\n",
    "scores = data.column(\"review_overall\")\n",
    "labels = [label_class(score, thresh_neg, thresh_neu) for score in scores] \n",
    "\n",
    "# add to data\n",
    "data = data.with_column(\"class\", labels)\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of the resulting classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we again develop different models to score the sentiment of a beer review. As before, split the dataset into a training set (80%) and test set (20%). Recall that the training set will be used to develop the model, while the test set will be used to evaluate the model. This allows us to more reliably evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 123\n",
    "\n",
    "train, test = train_test_split(data.to_df(), test_size=0.20, random_state=seed)\n",
    "train = Table.from_df(train)\n",
    "test = Table.from_df(test)\n",
    "\n",
    "x_train = train.column(\"review_text\")\n",
    "y_train = train.column(\"class\")\n",
    "scores_train = train.column(\"review_overall\")\n",
    "\n",
    "x_test = test.column(\"review_text\")\n",
    "y_test = test.column(\"class\")\n",
    "scores_test = test.column(\"review_overall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the 100-dimensional GloVe embeddings we worked with in labs 08 and 09. Recall that these are trained using word cooccurrence counts from about 6 billion words of text from Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gdl\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove = gdl.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the functions we've been using for preprocessing of documents. Recall that we also lemmatize words (verbs) to derive their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def is_numeric(string):\n",
    "    return any(char.isdigit() for char in string)\n",
    "\n",
    "def has_poss_contr(string):\n",
    "    return '\\'s' in string\n",
    "\n",
    "def empty_string(string):\n",
    "    return not string\n",
    "\n",
    "def remove_string(string):\n",
    "    return is_numeric(string) or has_poss_contr(string) or empty_string(string)\n",
    "\n",
    "def preprocess_data(docs):\n",
    "    docs = [re.sub(r'[^\\w\\s]', '', doc) for doc in docs]\n",
    "    docs_tok = [doc.lower().strip().split(' ') for doc in docs]\n",
    "    docs_tok = [[tok for tok in doc if not remove_string(tok)] for doc in docs_tok]\n",
    "    docs_tok = [[lemmatizer.lemmatize(tok, pos='v') for tok in doc] for doc in docs_tok]\n",
    "    return docs_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a couple more utility functions. The first filters a list of `tokens` and only keeps those that are in `vocab`. This helps us deal with tokens that may be out of GloVe's vocabulary by removing them. The second (from the previous lab) uses a model to predict the classes of reviews in `features`, and calculates the accuracy against `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_not_in_vocab(tokens, vocab):\n",
    "    return [token for token in tokens if token in vocab]\n",
    "\n",
    "def evaluate(model, features, labels, split=None):\n",
    "    pred_class = model.predict(features)\n",
    "    acc = np.mean(np.equal(labels, pred_class))\n",
    "    print(\"Classification accuracy (%s): %f\" % (split, acc))\n",
    "    return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Document Embeddings by Taking the Mean of Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple approach to obtaining document embeddings is to average out all the word embeddings for each token in the document.\n",
    "\n",
    "$$\n",
    "    a = \\frac{1}{|D|}\\sum_{w \\in D} \\phi(w)\n",
    "$$\n",
    "\n",
    "The result will be stored in a matrix, a data structure that stores an array of arrays.\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "        a_1 & a_2 & \\dots & a_d \\\\\n",
    "        b_1 & b_2 & \\dots & b_d \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        c_1 & c_2 & \\dots & c_d \\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Each row in this matrix corresponds to a document embedding (a list of numbers `a = [a_1,a_2,...,a_d]`). Each embedding will, in our case, be of size $d=100$ since we average over 100-dimensional GloVe vectors. This will be the vector representation of a review. So, the matrix has $n$ rows and $d = 100$ columns, where $n$ is the number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_emb_avg(docs, emb_model):\n",
    "    \n",
    "    # vocab\n",
    "    if emb_model == glove:\n",
    "        vocab = glove.vocab\n",
    "    elif emb_model == w2v:\n",
    "        vocab = w2v.wv.vocab\n",
    "    \n",
    "    # preprocess data and filter tokens not in  vocab\n",
    "    docs_tok = preprocess_data(docs)\n",
    "    docs_tok = [filter_not_in_vocab(docs, glove.vocab) for docs in docs_tok]\n",
    "    \n",
    "    # function to average vectors\n",
    "    def average_vectors(tokens):\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            vectors.append(emb_model[token])\n",
    "        return np.mean(vectors, axis=0)\n",
    "    \n",
    "    # get a list of document vectors\n",
    "    docs_emb = [average_vectors(tokens) for tokens in docs_tok]\n",
    "    \n",
    "    # stack document vectors into matrix\n",
    "    docs_emb = np.vstack(docs_emb)\n",
    "    \n",
    "    return docs_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we compute the document embeddings for `x_train` and `x_test` respectively. We also scale the embeddings so that they have mean 0 and variance 1; this is also called \"standardization.\" It's a typical step in many machine learning methods so they will behave well, and we've also seen it for many statistical procedures in YData.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document embeddings \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "docs_emb_train = get_doc_emb_avg(x_train, glove)\n",
    "docs_emb_test = get_doc_emb_avg(x_test, glove)\n",
    "\n",
    "# scale the document embeddings \n",
    "x_train_feat = scaler.fit_transform(docs_emb_train)\n",
    "x_test_feat = scaler.transform(docs_emb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our classifier. The first classifier we will consider is logistic regression. The simpler case is to predict among two classes (binary). \n",
    "\n",
    "$$\n",
    "logit(\\hat{p}) = \\hat{\\beta_0} + \\hat{\\beta_{1}}x_1 + ... + \\hat{\\beta_{100}}x_{100}\n",
    "$$\n",
    "\n",
    "In this case, we learn the coefficients $\\beta$, just as we would for linear regression. Taking the weighted sum of components of the document embedding, weighting component $x_j$ by $\\beta_j$, gives the log odds of the document belonging to a class.\n",
    "\n",
    "In our case, we predict among 3 possible classes, so we have to extend binary logistic regression to multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train multinomial logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, multi_class='multinomial', \n",
    "                                solver='sag', max_iter=100)\n",
    "model = clf.fit(x_train_feat, y_train)\n",
    "\n",
    "# evaluate model on train set\n",
    "preds_train = evaluate(model, x_train_feat, y_train, split=\"train\")\n",
    "\n",
    "# evaluate model on test set\n",
    "preds_test = evaluate(model, x_test_feat, y_test, split=\"test\")\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix  \n",
    "# print(confusion_matrix(y_test, preds_test))  \n",
    "# print(classification_report(y_test, preds_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression gives us 0.555 accuraccy over the test set. Not bad!\n",
    "\n",
    "We also try a k-NN classifier.\n",
    "\n",
    "<img width=350 src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/Knn_k1_z96jba.png\">\n",
    "\n",
    "The idea of the k-NN classifier is to find the nearest k neighbors to a document in the vector space. The majority label among these k neighbors becomes the label of that document. We'll start with 1 nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "clf = KNeighborsClassifier(n_neighbors=1)  \n",
    "model = clf.fit(x_train_feat, y_train) \n",
    "\n",
    "# evaluate model on train set\n",
    "preds_train = evaluate(model, x_train_feat, y_train, split=\"train\")\n",
    "\n",
    "# evaluate model on test set\n",
    "preds_test = evaluate(model, x_test_feat, y_test, split=\"test\")\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix  \n",
    "# print(confusion_matrix(y_test, preds_test))  \n",
    "# print(classification_report(y_test, preds_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-NN classifier doesn't perform as well as the logistic regression model. Can you suggest any reasons for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn! (1/2)\n",
    "\n",
    "### Evaluating different k\n",
    "\n",
    "Currently, we set $k=1$ so we effectively have a nearest neighbor classifier. How would performance change as we vary $k$?\n",
    "\n",
    "Plot the training accuracy and test acccuracy of a k-NN classifier againt $k$. You want two lines in your plot.  You can use the following outline to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define arrays for storing accuracies\n",
    "...\n",
    "\n",
    "# define how we vary neighbours; don't change this!\n",
    "neighbors = np.arange(1, 50, 2)\n",
    "for k in neighbors:\n",
    "    \n",
    "    # train a KNN classifier with param k\n",
    "    ...\n",
    "\n",
    "    # Note that this gives you the accuracy:\n",
    "    # 100*sum(y_train==preds_train)/len(preds_train)\n",
    "\n",
    "    # evaluate model on training set\n",
    "    ...\n",
    "    \n",
    "    # evaluate model on test set\n",
    "    ...\n",
    "    \n",
    "# plot both relationships on a graph\n",
    "# x axis: neighbours\n",
    "# y axis: training accuracy, test accuracy\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Document Embeddings using Weighted Sum of  Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to obtain document embeddings is to take a *weighted* sum of word embeddings. Intuitively, some words will be more important the others&mdash;this is why we selected lexicons of positive and negative words in the last lab. But how  should we weight the word embeddings? \n",
    "\n",
    "We can use tf-idf as weights. tf stands for *term frequency*, and idf stands for *inverse document frequency*. This is a statistical measure of how important a word is to a particular document among a large corpus of documents. \n",
    "\n",
    "Intuitively, the tf term reflects the idea that the importance of a word increases if it appears in a document more times. \n",
    "\n",
    "$$\n",
    "    \\text{tf}_D(w) = \\frac{\\text{# appearances of w in document D}}{\\text{# words in D}}\n",
    "$$\n",
    "\n",
    "The idf term reflects the idea that the importance of a word decreases if it appears across many documents; e.g., `the` will appear in almost all documents, and so it's not informative.\n",
    "\n",
    "$$\n",
    "    \\text{idf}(w) = \\log\\left(\\frac{\\text{# documents}}{\\text{# documents that contain w}}\\right)\n",
    "$$\n",
    "\n",
    "The tf-idf weight of a term is then just the product of tf and idf:\n",
    "\n",
    "$$\n",
    "    \\text{tf-idf}_D(w) = tf(w) \\times idf(w)\n",
    "$$\n",
    "\n",
    "We use this tf-idf weight for word w to form a weighted sum of the GloVe embedding vectors to obtain a document embedding:\n",
    "\n",
    "$$\n",
    "    a = \\sum_{w \\in D} \\text{tf-idf}_D(w) \\times \\phi(w)\n",
    "$$\n",
    "\n",
    "As before, we store document embeddings in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.matutils import sparse2full\n",
    "    \n",
    "def get_doc_emb_tfidf(docs, emb_model):\n",
    "    \n",
    "    # vocab\n",
    "    if emb_model == glove:\n",
    "        vocab = glove.vocab\n",
    "    elif emb_model == w2v:\n",
    "        vocab = w2v.wv.vocab\n",
    "    \n",
    "    # build dictionary (vocabulary) over training set\n",
    "    docs_tok = preprocess_data(docs)\n",
    "    docs_tok = [filter_not_in_vocab(docs, vocab) for docs in docs_tok]\n",
    "    docs_dict = Dictionary(docs_tok)\n",
    "    docs_dict.filter_extremes(no_below=35, no_above=0.08)\n",
    "    docs_dict.compactify()\n",
    "\n",
    "    # build tfidf matrix for tokens in dictionary\n",
    "    docs_corpus = [docs_dict.doc2bow(doc) for doc in docs_tok]\n",
    "    model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)\n",
    "    docs_tfidf  = model_tfidf[docs_corpus]\n",
    "    docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])\n",
    "\n",
    "    # build matrix of glove vectors for each token\n",
    "    tfidf_emb_vecs = np.vstack([emb_model[docs_dict[i]] for i in range(len(docs_dict))])\n",
    "    \n",
    "    # build document vectors, weighted by tfidf\n",
    "    docs_emb = np.dot(docs_vecs, tfidf_emb_vecs) \n",
    "    return docs_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the embeddings as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document embeddings \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_full = np.concatenate((x_train, x_test), axis=0)\n",
    "docs_emb = get_doc_emb_tfidf(x_full, glove)\n",
    "\n",
    "# we have to split into the embeddings for train docs and for test docs\n",
    "docs_emb_train = docs_emb[:train.num_rows,]\n",
    "docs_emb_test = docs_emb[train.num_rows:,] \n",
    "\n",
    "# scale embeddings\n",
    "x_train_feat = scaler.fit_transform(docs_emb_train)\n",
    "x_test_feat = scaler.transform(docs_emb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train new classifiers using logistic regression and k-NN classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train multinomial logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, multi_class='multinomial', \n",
    "                         solver='sag', max_iter=100)\n",
    "model = clf.fit(x_train_feat, y_train)\n",
    "\n",
    "# evaluate model on train set\n",
    "preds_train = evaluate(model, x_train_feat, y_train, split=\"train\")\n",
    "\n",
    "# evaluate model on test set\n",
    "preds_test = evaluate(model, x_test_feat, y_test, split=\"test\")\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix  \n",
    "# print(confusion_matrix(y_test, preds_test))  \n",
    "# print(classification_report(y_test, preds_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we say about the accuracy compared with the previous logistic regression model? Can you think of any possible explanation for this? How might we try to improve the model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll train the k-NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "clf = KNeighborsClassifier(n_neighbors=7)  \n",
    "model = clf.fit(x_train_feat, y_train) \n",
    "\n",
    "# evaluate model on train set\n",
    "preds_train = evaluate(model, x_train_feat, y_train, split=\"train\")\n",
    "\n",
    "# evaluate model on test set\n",
    "preds_test = evaluate(model, x_test_feat, y_test, split=\"test\")\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix  \n",
    "# print(confusion_matrix(y_test, preds_test))  \n",
    "# print(classification_report(y_test, preds_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embeddings using Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last method we will consider for document embeddings is called Doc2Vec. The details are not necessary here, but the algorithm is similar to Word2Vec. This takes a while to run, so we can simply load from a previously saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "x_full = np.concatenate((x_train, x_test), axis=0)\n",
    "\n",
    "docs = preprocess_data(x_full)\n",
    "tagged_docs = [TaggedDocument(words=d, tags=[str(i)]) for i, d in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train d2v model\n",
    "# max_epochs = 100\n",
    "# vec_size = 100\n",
    "# alpha = 0.025\n",
    "\n",
    "# d2v = Doc2Vec(size=vec_size,\n",
    "#               alpha=alpha, \n",
    "#               min_alpha=0.00025,\n",
    "#               min_count=1,\n",
    "#               dm=1)\n",
    "  \n",
    "# d2v.build_vocab(tagged_docs)\n",
    "\n",
    "# for epoch in range(max_epochs):\n",
    "#     if epoch % 10 == 0:\n",
    "#         print('iteration {0}'.format(epoch))\n",
    "#     d2v.train(tagged_docs,\n",
    "#                 total_examples=d2v.corpus_count,\n",
    "#                 epochs=model.iter)\n",
    "#     # decrease the learning rate\n",
    "#     d2v.alpha -= 0.0002\n",
    "#     # fix the learning rate, no decay\n",
    "#     d2v.min_alpha = d2v.alpha\n",
    "\n",
    "# d2v.save(\"100-d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = Doc2Vec.load(\"100-d2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, scale the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# build the matrix\n",
    "docs_emb = np.vstack([d2v.docvecs[str(i)] for i in range(data.num_rows)])\n",
    "\n",
    "# we have to split into the embeddings for train docs and for test docs\n",
    "docs_emb_train = docs_emb[:train.num_rows,]\n",
    "docs_emb_test = docs_emb[train.num_rows:,] \n",
    "\n",
    "# scale embeddings\n",
    "x_train_feat = scaler.fit_transform(docs_emb_train)\n",
    "x_test_feat = scaler.transform(docs_emb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train multinomial logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, multi_class='multinomial', \n",
    "                         solver='sag', max_iter=100)\n",
    "model = clf.fit(x_train_feat, y_train)\n",
    "\n",
    "# evaluate model on train set\n",
    "preds_train = evaluate(model, x_train_feat, y_train, split=\"train\")\n",
    "\n",
    "# evaluate model on test set\n",
    "preds_test = evaluate(model, x_test_feat, y_test, split=\"test\")\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix  \n",
    "# print(confusion_matrix(y_test, preds_test))  \n",
    "# print(classification_report(y_test, preds_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs well, with 0.591 accuracy on the test set. \n",
    "\n",
    "Train a KNN classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train KNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "clf = KNeighborsClassifier(n_neighbors=1)  \n",
    "model = clf.fit(x_train_feat, y_train) \n",
    "\n",
    "# evaluate model on train set\n",
    "preds_train = evaluate(model, x_train_feat, y_train, split=\"train\")\n",
    "\n",
    "# evaluate model on test set\n",
    "preds_test = evaluate(model, x_test_feat, y_test, split=\"test\")\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix  \n",
    "# print(confusion_matrix(y_test, preds_test))  \n",
    "# print(classification_report(y_test, preds_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to do poorly once again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn! (2/2)\n",
    "\n",
    "### Word2Vec embeddings\n",
    "\n",
    "Currently, we use GloVe word embeddings to build document embeddings. How might the performance change if we use Word2Vec embeddings constructed from the training set instead?\n",
    "\n",
    "First, train word embeddings on `x_train` by filling in the following chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "\n",
    "... = preprocess_data(...)\n",
    "w2v = word2vec.Word2Vec(..., size=100, window=10, iter=10, min_count=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, get document embeddings via the first method (mean of word embeddings). Then, scale the document embeddings. Your code for the following chunks should look like what has been given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document embeddings\n",
    "\n",
    "# scale document embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, train a logistic regression model, and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, train a KNN classifier model, and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate KNN classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the performance of the models. How do they compare with the GloVe embeddings?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
