{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 08: Word embeddings (1/2)\n",
    "\n",
    "<br>\n",
    "\n",
    "[<img width=900 src=\"ginsparg.png\">](http://www.cs.cornell.edu/~ginsparg/arxiv/gmaps2.html)\n",
    "\n",
    "Word embeddings are algorithms that represent categorical data like words as vectors in a high dimensional space. \n",
    "These are machine learning methods that construct the embedding vectors using cooccurrence statistics, expressed in terms of simple language models. Embeddings reveal surprising semantic relations encoded in linear relationships. But they are \"data hungry\" and require large corpora of text or other coocurrence data to construct good embeddings. \n",
    "\n",
    "In this lab we will explore some of the basics of word embeddings, playing around with two types of embeddings constructed on large amounts of text extracted from [Wikipedia](https://en.wikipedia.org/wiki/Main_Page). There are several tutorials on the web for this material; one is [here](https://medium.com/swlh/playing-with-word-vectors-308ab2faa519).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, load in the usual modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "import os\n",
    "# this turns off some pesky warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=Warning)\n",
    "\n",
    "# direct plots to appear within the cell, and set their style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [gensim package](https://radimrehurek.com/gensim/index.html), already familiar to us from our foray into topic models. The following bit of code reads in 100-dimensional embedding vectors, trained using the [GloVe](https://nlp.stanford.edu/projects/glove/) algorithm on a collection of Wikipedia data. Specifically, it uses 6 billion tokens of Wikipedia, with a 400,000 word vocabulary. You can find other precompiled embeddings [here](https://www.diycode.cc/projects/RaRe-Technologies/gensim-data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gdl\n",
    "from gensim.models import KeyedVectors\n",
    "glove = gdl.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore these embeddings a bit. Here is the vector for 'yale'. Pretty interesting, huh?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove['yale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now let's see which vectors are closest to the 'yale' vector. This is a little more interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glove.most_similar('yale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explore word similarity using embeddings.  \n",
    "\n",
    "Now, create a few cells where you use the `most_similar` function to find similar words to a few words that you select. Add some markdown to describe your findings, and why they do and do not seem to make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and markdown go here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some of the components of the embedding vectors. What do the distributions of values look like?\n",
    "We'll first pull out the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = set([w for w in glove.vocab])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 50 # we'll look at this component\n",
    "\n",
    "x = [] # this will be a list of all 400,000 values, one for each word in the vocabulary\n",
    "for w in vocab:\n",
    "    x.append(glove[w][i])\n",
    "\n",
    "ax = sns.distplot(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate a scatter plot\n",
    "\n",
    "Now, generate a scatter plot of a few <i>pairs</i> of components. For example, you could extract the first and second components of all the embedding vectors. What do you see? Describe the distributions of points. Do they look random?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and markdown go here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, next we are going to construct a new set of embedding vectors, on a smaller subset of Wikipedia. The following code runs the \"word2vec\" algorithm to make embeddings. It takes a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "# !wget http://mattmahoney.net/dc/text8.zip\n",
    "# !unzip text8.zip\n",
    "sentences = word2vec.Text8Corpus('text8')\n",
    "w2v = word2vec.Word2Vec(sentences, size=100, window=10, min_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v.most_similar('yale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare the embeddings\n",
    "\n",
    "Choose five words, and for each word look at the most similar words according to the `glove` and the `w2v` embeddings. What do you find? Describe at least two reasons why one set of embeddings might be better than the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and markdown go here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exploring analogies\n",
    "\n",
    "Now we'll explore how analogies are \"solved\" using the embeddings. Here is the canonical example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glove.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to get the top choice, you can just do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glove.most_similar(positive=['king', 'woman'], negative=['man'])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar(positive=['king', 'woman'], negative=['man'])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now choose at least five analogies. For each one, try to complete them with both the `glove` and the `w2v` model. It will be best if you write some code to do this as you loop over a list of triples of words, like ('king', 'man', 'woman').\n",
    "\n",
    "Which of the analogies do the models \"get right\"? Which are clearly wrong? Describe your findings and speculate on some reasons that the models might miss some of the analogies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and markdown go here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
